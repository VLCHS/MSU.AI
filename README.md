# MSU.AI

Методы машинного обучения могут стать наиболее удобным инструментом для экстраполяции/интерполяции экспериментальных данных в физике частиц. В данной работе представлены результаты использования ансамблевых методов (случайный лес и XGBoost) и полносвязной нейронной сети для предсказания дифференциальных сечений. В центре внимания реакция взаимодействия электрона с протоном с образованием нейтрального пи-мезона: $e^{-}$ p -> $e^{-}$ p $\pi^{0}$, однако полученые модели могут быть применены и для анализа реакции $e^{-}$ p -> $e^{-}$ n $\pi^{+}$

# Комментарии к файлам в репозитории  

Final_Project.ipynb - файл с кодом обучения 

clasdb_pi_0_p - часть экспериментальных данных (5 000 событий) реакции ep->ep_pi0

реакция с нейтроном дополнительная, код с файлом clasdb_pi_plus_n.txt следует пропустить

# Обучение

! Для обучения ансамблевых методов используются параметры по умолчанию. Подбор оптимальных параметров осуществляется с использованием библиотеки Optuna, для экономии времении можно выбрать меньшее число в n_trials.

! Для обучения нейронной сети необходимо ввести ключ для логирования в wandb и настроить гиперпараметры (можно использовать уже указанные в файле, они являются одними из наиболее оптимальных):

- 'scale_data' - флаг, указывающий на использование скейлинга для признаков (feature_scaler) и целевой переменной (label_scaler)

- 'augment' - флаг, указывающий на использование аугментации данных при помощи вариационного автоэнкодера

- 'net_architecture' - показывает количество нейронов в каждом слое полносвязной сети. Последний слой (1 нейрон) не меняется, количество нейронов в первом слое зависит от задачи. В данном случае фиксируется значение 6
    
- 'activation_function' - функция активации, лучше всего использовать разновидности ReLU

- 'loss_func' - функция потерь

- 'optim_func'- оптимизатор

- 'max_epochs' - максимальное количество эпох для обучения сети

- 'es_min_delta' и 'es_patience' - параметры для ранней остановки обучения. По умолчанию рання остановка отсутствует и сеть обучается ровно 150 эпох

- 'lr','lr_factor', 'lr_patience', 'lr_cooldown' - параметры для скорости обучения. По умолчанию: если ошибка MAE на валидационной выборке не снижается в течение 5 эпох подряд после 20 эпох отсутствия мониторинга, скорость обучения сокращается в 2 раза, её начальное значение равно 0.001

! Для экономии времени строятся не все графики распределения дифференциальных сечений


